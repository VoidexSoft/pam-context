"""
LLM-as-judge scoring module for PAM Context evaluation.

Uses the Anthropic SDK to score agent-generated answers against expected answers
on three dimensions: factual accuracy, citation presence, and completeness.
"""

import os
import json
import anthropic

RUBRIC_PROMPT = """\
You are an expert evaluator for a business knowledge Q&A system. Your job is to
score an AI-generated answer against a reference (expected) answer.

You will evaluate on three dimensions, each scored from 0.0 to 1.0:

1. **Factual Accuracy** (0-1): Does the actual answer contain correct facts
   compared to the expected answer? Penalize hallucinated facts, wrong numbers,
   or incorrect definitions. A score of 1.0 means every factual claim aligns
   with the expected answer. A score of 0.0 means the answer is entirely wrong
   or fabricated.

2. **Citation Presence** (0-1): Does the actual answer reference specific
   sources, documents, tables, or sections? A score of 1.0 means every major
   claim is backed by a citation (document name, section, table, URL, or SQL
   query). A score of 0.5 means some citations are present but not all claims
   are cited. A score of 0.0 means no citations at all.

3. **Completeness** (0-1): Does the actual answer cover all the key points in
   the expected answer? A score of 1.0 means all key points are addressed.
   A score of 0.5 means roughly half the key points are covered. A score of
   0.0 means the answer is empty or misses all key points.

Respond ONLY with a valid JSON object in the following format (no markdown, no
explanation, just the JSON):

{
  "factual_accuracy": <float 0-1>,
  "citation_presence": <float 0-1>,
  "completeness": <float 0-1>,
  "reasoning": "<brief explanation of the scores>"
}
"""


async def score_answer(
    question: str,
    expected_answer: str,
    actual_answer: str,
    model: str = "claude-sonnet-4-5-20250514",
) -> dict:
    """
    Score an actual answer against an expected answer using LLM-as-judge.

    Args:
        question: The original question asked.
        expected_answer: The reference/gold answer.
        actual_answer: The answer generated by the system under evaluation.
        model: The Anthropic model to use for judging.

    Returns:
        A dict with keys: factual_accuracy, citation_presence, completeness,
        average_score, and reasoning.
    """
    client = anthropic.AsyncAnthropic(
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
    )

    user_message = (
        f"## Question\n{question}\n\n"
        f"## Expected Answer\n{expected_answer}\n\n"
        f"## Actual Answer\n{actual_answer}\n\n"
        "Please score the actual answer according to the rubric."
    )

    response = await client.messages.create(
        model=model,
        max_tokens=512,
        system=RUBRIC_PROMPT,
        messages=[{"role": "user", "content": user_message}],
    )

    raw_text = response.content[0].text.strip()

    try:
        scores = json.loads(raw_text)
    except json.JSONDecodeError:
        # Try to extract JSON from the response if it has extra text
        import re

        match = re.search(r"\{.*\}", raw_text, re.DOTALL)
        if match:
            scores = json.loads(match.group())
        else:
            return {
                "factual_accuracy": 0.0,
                "citation_presence": 0.0,
                "completeness": 0.0,
                "average_score": 0.0,
                "reasoning": f"Failed to parse judge response: {raw_text[:200]}",
            }

    factual = float(scores.get("factual_accuracy", 0.0))
    citation = float(scores.get("citation_presence", 0.0))
    completeness = float(scores.get("completeness", 0.0))
    avg = round((factual + citation + completeness) / 3, 4)

    return {
        "factual_accuracy": factual,
        "citation_presence": citation,
        "completeness": completeness,
        "average_score": avg,
        "reasoning": scores.get("reasoning", ""),
    }
