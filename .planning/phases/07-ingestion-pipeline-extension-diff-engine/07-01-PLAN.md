---
phase: 07-ingestion-pipeline-extension-diff-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - alembic/versions/006_add_graph_synced.py
  - src/pam/common/models.py
  - src/pam/ingestion/stores/postgres_store.py
  - src/pam/graph/extraction.py
  - src/pam/ingestion/diff_engine.py
  - src/pam/graph/service.py
  - src/pam/graph/__init__.py
autonomous: true
requirements:
  - EXTRACT-01
  - EXTRACT-02
  - EXTRACT-03
  - EXTRACT-06
  - DIFF-01
  - DIFF-02

must_haves:
  truths:
    - "Document model has graph_synced boolean and graph_sync_retries integer columns"
    - "Alembic migration 006 creates graph_synced column with index and graph_sync_retries column"
    - "extract_graph_for_document() calls add_episode() once per chunk with group_id, entity_types, and reference_time"
    - "Chunk-level diff engine compares old vs new segment content_hash sets and classifies chunks as added/removed/unchanged"
    - "Diff engine removes stale episodes via remove_episode() before adding new/changed chunk episodes"
    - "Episode UUIDs are stored in segment metadata_ for surgical cleanup on re-ingestion"
    - "Rollback function removes all successfully-added episodes on partial failure"
    - "Diff summary includes added/removed entities with full detail persisted as structured JSON"
  artifacts:
    - path: "alembic/versions/006_add_graph_synced.py"
      provides: "Database migration for graph_synced and graph_sync_retries columns"
      contains: "graph_synced"
    - path: "src/pam/graph/extraction.py"
      provides: "Graph extraction orchestrator with add_episode per chunk and rollback"
      exports: ["extract_graph_for_document", "rollback_graph_for_document"]
    - path: "src/pam/ingestion/diff_engine.py"
      provides: "Chunk-level diff engine and entity-level diff summary builder"
      exports: ["ChunkDiff", "compute_chunk_diff", "build_diff_summary"]
    - path: "src/pam/common/models.py"
      provides: "Updated Document model with graph_synced fields"
      contains: "graph_synced"
    - path: "src/pam/ingestion/stores/postgres_store.py"
      provides: "Methods for graph sync flag management and segment retrieval"
      exports: ["set_graph_synced", "get_unsynced_documents", "get_segments_for_document"]
  key_links:
    - from: "src/pam/graph/extraction.py"
      to: "graphiti_core.Graphiti.add_episode"
      via: "graph_service.client.add_episode() call per chunk"
      pattern: "add_episode"
    - from: "src/pam/graph/extraction.py"
      to: "src/pam/ingestion/diff_engine.py"
      via: "compute_chunk_diff() call for re-ingestion"
      pattern: "compute_chunk_diff"
    - from: "src/pam/graph/extraction.py"
      to: "src/pam/graph/entity_types.py"
      via: "ENTITY_TYPES passed to add_episode"
      pattern: "ENTITY_TYPES"
---

<objective>
Build the graph extraction orchestrator, chunk-level diff engine, and database foundation for graph sync tracking.

Purpose: This plan creates the core modules that subsequent pipeline integration will call. The extraction orchestrator handles per-chunk episode ingestion into Graphiti with bi-temporal timestamps, the diff engine enables cost-effective re-ingestion by only processing changed chunks, and the Alembic migration adds the graph_synced tracking column.

Output: Alembic migration 006, updated Document model, graph extraction module with add_episode/rollback, diff engine with chunk comparison and entity summary builder, and PostgresStore methods for sync flag management.
</objective>

<execution_context>
@/Users/datnguyen/.claude/get-shit-done/workflows/execute-plan.md
@/Users/datnguyen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-ingestion-pipeline-extension-diff-engine/07-RESEARCH.md
@.planning/phases/06-neo4j-graphiti-infrastructure/06-01-SUMMARY.md
@.planning/phases/06-neo4j-graphiti-infrastructure/06-02-SUMMARY.md
@src/pam/graph/service.py
@src/pam/graph/entity_types.py
@src/pam/common/models.py
@src/pam/ingestion/stores/postgres_store.py
@alembic/versions/005_add_content_hash_index_and_role_constraint.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Alembic migration + Document model + PostgresStore methods</name>
  <files>
    alembic/versions/006_add_graph_synced.py
    src/pam/common/models.py
    src/pam/ingestion/stores/postgres_store.py
  </files>
  <action>
    **Alembic migration (006_add_graph_synced.py):**
    Create a new Alembic migration following the existing pattern (see 004, 005 for style):
    - Add `graph_synced` Boolean column to `documents` table with `server_default=sa.text("false")`, `nullable=False`
    - Add `graph_sync_retries` Integer column to `documents` table with `server_default=sa.text("0")`, `nullable=False`
    - Create index `ix_documents_graph_synced` on the `graph_synced` column
    - Use `op.add_column()` and `op.create_index()` pattern
    - Include proper `downgrade()` that drops index and columns in reverse order
    - Revises the previous migration (005)

    **Document model update (models.py):**
    Add two new fields to the `Document` class after the existing `status` field:
    ```python
    graph_synced: Mapped[bool] = mapped_column(Boolean, server_default=text("false"), nullable=False)
    graph_sync_retries: Mapped[int] = mapped_column(Integer, server_default=text("0"), nullable=False)
    ```
    Import `Boolean` and `text` from sqlalchemy if not already imported.
    Also update `DocumentResponse` Pydantic model to include `graph_synced: bool = False`.

    **PostgresStore methods:**
    Add these methods to the `PostgresStore` class:

    1. `async def set_graph_synced(self, document_id: uuid.UUID, synced: bool, increment_retries: bool = False) -> None:`
       - Updates `Document.graph_synced` for the given doc_id
       - If `increment_retries=True`, also increments `graph_sync_retries` by 1
       - If `synced=True`, resets `graph_sync_retries` to 0
       - Uses `update(Document).where(Document.id == document_id).values(...)`
       - Calls `flush()` after

    2. `async def get_unsynced_documents(self, max_retries: int = 3, limit: int | None = None) -> list[Document]:`
       - Queries documents where `graph_synced == False` and `graph_sync_retries < max_retries`
       - Applies optional `limit`
       - Returns list of Document objects

    3. `async def get_segments_for_document(self, document_id: uuid.UUID) -> list[Segment]:`
       - Queries all Segment rows for the given document_id, ordered by position
       - Returns list of Segment ORM objects (needed by diff engine to compare old vs new)

    Import `update` from sqlalchemy (add to existing import line).
  </action>
  <verify>
    - Run `ruff check src/pam/common/models.py src/pam/ingestion/stores/postgres_store.py` -- no lint errors
    - Run `python -c "from pam.common.models import Document; print(hasattr(Document, 'graph_synced'))"` -- prints True
    - Run `python -c "from pam.ingestion.stores.postgres_store import PostgresStore; print(hasattr(PostgresStore, 'set_graph_synced'))"` -- prints True
    - Verify migration file exists and has both upgrade/downgrade functions
  </verify>
  <done>
    Document model has graph_synced (bool) and graph_sync_retries (int) columns. Alembic migration 006 creates both columns with proper defaults and index. PostgresStore has set_graph_synced(), get_unsynced_documents(), and get_segments_for_document() methods.
  </done>
</task>

<task type="auto">
  <name>Task 2: Graph extraction orchestrator + chunk-level diff engine</name>
  <files>
    src/pam/graph/extraction.py
    src/pam/ingestion/diff_engine.py
    src/pam/graph/service.py
    src/pam/graph/__init__.py
  </files>
  <action>
    **Diff engine (src/pam/ingestion/diff_engine.py):**
    Create a new module with these components:

    1. `ChunkDiff` dataclass:
       ```python
       @dataclass
       class ChunkDiff:
           added: list[Any]        # New segments not in old set (KnowledgeSegment)
           removed: list[Any]      # Old segments not in new set (Segment ORM objects)
           unchanged: list[Any]    # Same content_hash in both sets (KnowledgeSegment)
       ```

    2. `compute_chunk_diff(old_segments: list[Segment], new_segments: list[KnowledgeSegment]) -> ChunkDiff`:
       - Build `old_hashes = {seg.content_hash: seg for seg in old_segments}`
       - Build `new_hashes = {seg.content_hash: seg for seg in new_segments}`
       - `added`: new_segments whose content_hash is NOT in old_hashes
       - `removed`: old_segments whose content_hash is NOT in new_hashes
       - `unchanged`: new_segments whose content_hash IS in old_hashes
       - For unchanged segments, copy over `graph_episode_uuid` from old segment's `metadata_` to new segment's `metadata` dict (preserves episode tracking)
       - Return ChunkDiff

    3. `build_diff_summary(added_entities: list[dict], removed_episode_uuids: list[str], old_entity_names: set[str], new_entity_names: set[str]) -> dict`:
       - Returns structured JSON dict with keys:
         - `added`: entities in new_entity_names but not old_entity_names (list of dicts with name, type)
         - `removed_from_document`: entities in old_entity_names but not new_entity_names
         - `episodes_added`: count of new episodes
         - `episodes_removed`: count of removed episodes
       - Per user decision: distinguish "removed from document" vs "deleted from graph" -- the `removed_from_document` field captures document-scoped removal. Full graph deletion is determined by Graphiti's `remove_episode()` behavior (entities survive if referenced by other episodes).

    **Graph extraction orchestrator (src/pam/graph/extraction.py):**
    Create a new module with these components:

    1. `@dataclass ExtractionResult`:
       - `episodes_added: int`
       - `episodes_removed: int`
       - `entities_extracted: list[dict]` (name, type for each entity)
       - `diff_summary: dict` (the structured diff for SyncLog)
       - `progress_log: list[str]` (e.g., "extracting entities 3/10 chunks")

    2. `async def extract_graph_for_document(graph_service: GraphitiService, doc_id: uuid.UUID, segments: list[KnowledgeSegment], document_title: str, reference_time: datetime, source_id: str, old_segments: list[Segment] | None = None) -> ExtractionResult`:
       - If `old_segments` is provided and non-empty, compute chunk diff via `compute_chunk_diff(old_segments, segments)`
       - Otherwise, treat all segments as `added` (first ingestion)
       - **Phase 1 -- Remove stale episodes** (for re-ingestion):
         - For each `removed` segment in the diff, get `graph_episode_uuid` from `segment.metadata_`
         - Call `await graph_service.client.remove_episode(episode_uuid)` for each
         - Collect removed UUIDs
       - **Phase 2 -- Extract new/changed chunks**:
         - For each `added` segment:
           - Call `await graph_service.client.add_episode(name=f"chunk-{segment.id}", episode_body=segment.content, source=EpisodeType.text, source_description=f"Document: {document_title} | Source: {source_id} | Chunk: {segment.position}", reference_time=reference_time, group_id=f"doc-{doc_id}", entity_types=ENTITY_TYPES)`
           - Store `result.episode.uuid` in `segment.metadata["graph_episode_uuid"]`
           - Store `len(result.nodes)` in `segment.metadata["graph_entity_count"]`
           - Collect extracted entity info (name, type from result.nodes labels)
           - Append progress log entry: `f"extracted {i+1}/{total_to_extract} chunks"`
         - Track successfully-added episode UUIDs for potential rollback
       - **Phase 3 -- Build diff summary** using `build_diff_summary()` from diff_engine
       - Return `ExtractionResult` with all collected data
       - IMPORTANT: Do NOT catch exceptions in this function. The caller (pipeline.py) handles the try/except for fault isolation. This function should propagate errors so the caller can trigger rollback.

    3. `async def rollback_graph_for_document(graph_service: GraphitiService, segments: list[KnowledgeSegment]) -> int`:
       - For each segment with a `graph_episode_uuid` in its metadata, call `remove_episode()`
       - Catch exceptions per-episode (log warning but continue rollback)
       - Clear the `graph_episode_uuid` from segment metadata after removal
       - Return count of successfully rolled-back episodes

    **GraphitiService update (service.py):**
    No changes needed to the service class itself -- extraction.py uses `graph_service.client` directly.

    **Update graph/__init__.py:**
    Add exports for `extract_graph_for_document`, `rollback_graph_for_document`, and `ExtractionResult` from `extraction` module.

    Import notes:
    - Use `from graphiti_core.nodes import EpisodeType` for the source parameter
    - Use `from pam.graph.entity_types import ENTITY_TYPES` for entity type constraint
    - Use `from pam.ingestion.diff_engine import ChunkDiff, compute_chunk_diff, build_diff_summary`
    - Type annotations: use `from __future__ import annotations` and TYPE_CHECKING block for GraphitiService to avoid circular imports
  </action>
  <verify>
    - Run `ruff check src/pam/graph/extraction.py src/pam/ingestion/diff_engine.py` -- no lint errors
    - Run `python -c "from pam.graph.extraction import extract_graph_for_document, rollback_graph_for_document, ExtractionResult; print('OK')"` -- prints OK
    - Run `python -c "from pam.ingestion.diff_engine import ChunkDiff, compute_chunk_diff, build_diff_summary; print('OK')"` -- prints OK
    - Run `python -c "from pam.graph import extract_graph_for_document, ExtractionResult; print('OK')"` -- imports from __init__ work
  </verify>
  <done>
    Graph extraction orchestrator calls add_episode() per chunk with group_id=f"doc-{doc_id}", entity_types=ENTITY_TYPES, and reference_time=document.modified_at. Episode UUIDs stored in segment metadata for later cleanup. Diff engine compares content_hash sets to classify chunks as added/removed/unchanged. Rollback function surgically removes episodes on failure. Diff summary builder produces structured JSON with added/removed entities for SyncLog persistence.
  </done>
</task>

</tasks>

<verification>
- All new Python files pass `ruff check` with no errors
- All imports resolve correctly (no circular imports)
- Document model has graph_synced and graph_sync_retries fields
- Migration 006 exists with correct upgrade/downgrade
- extraction.py uses add_episode() (never add_episode_bulk per requirements)
- extraction.py passes ENTITY_TYPES to constrain extraction
- diff_engine.py computes chunk-level diff via content_hash comparison
- Episode UUIDs tracked in segment metadata for surgical cleanup
</verification>

<success_criteria>
- Alembic migration 006 is valid and creates graph_synced + graph_sync_retries columns with index
- extract_graph_for_document() handles both first ingestion and re-ingestion with diff
- compute_chunk_diff() correctly classifies segments as added/removed/unchanged by content_hash
- rollback_graph_for_document() can clean up partial graph writes
- PostgresStore has methods for graph sync flag management
- All modules are importable without errors
</success_criteria>

<output>
After completion, create `.planning/phases/07-ingestion-pipeline-extension-diff-engine/07-01-SUMMARY.md`
</output>
